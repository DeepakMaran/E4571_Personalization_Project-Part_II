{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import surprise\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k, threshold):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "        \n",
    "        # Compute F-score\n",
    "        f_score = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "    return precision, recall, f_score\n",
    "\n",
    "\n",
    "def ndcg_at_k(predictions, k):\n",
    "    dcgs = dict()\n",
    "    idcgs = dict()\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        #estimated rank\n",
    "        rank_est = np.arange(1, len(user_ratings[:k])+1)\n",
    "        discount_est = np.log2(rank_est+1)\n",
    "        \n",
    "        #Relevance \n",
    "        rel = [np.power(2,true_r)-1 for (_, true_r) in user_ratings[:k]]\n",
    "        \n",
    "        dcgs[uid] = sum(rel/discount_est)\n",
    "        \n",
    "        # Sort user ratings by true value\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        #estimated rank\n",
    "        rank_true = np.arange(1, len(user_ratings[:k])+1)\n",
    "        discount_true = np.log2(rank_true+1)\n",
    "        \n",
    "        #Relevance \n",
    "        rel_true = [np.power(2,true_r)-1 for (_, true_r) in user_ratings[:k]]\n",
    "        \n",
    "        idcgs[uid] = sum(rel_true/discount_true)\n",
    "        \n",
    "    dcg = sum(dcgu for (_,dcgu) in dcgs.items())\n",
    "    idcg = sum(idcgu for (_,idcgu) in idcgs.items())\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def user_space_coverage(predictions, k, n_user, threshold):\n",
    "\t# First map the predictions to each user.\n",
    "    user_est = defaultdict(list)\n",
    "    for uid, _, _, est, _ in predictions:\n",
    "        if est >= threshold:\n",
    "            user_est[uid].append(est)\n",
    "    n_user_k = sum((len(n_est) >= k ) for n_est in user_est.values())\n",
    "    a = n_user_k/n_user\n",
    "    return a\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "        \n",
    "    return top_n\n",
    "\n",
    "def item_space_coverage(predictions, k, n_items, threshold):\n",
    "    top_n = get_top_n(predictions, k)\n",
    "    items = []\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for (iid, rtg) in user_ratings:\n",
    "            if rtg >= threshold:\n",
    "                items.append(iid)\n",
    "    \n",
    "    return(len(set(items))/n_items)\n",
    "\n",
    "# the items recommended to each user based on predictions\n",
    "def recommendation_list(predictions, k, threshold):\n",
    "    recom_list = defaultdict(list)\n",
    "    top_n = get_top_n(predictions, k)\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for (iid, rating) in user_ratings:\n",
    "            if rating >= threshold:\n",
    "                recom_list[uid].append(iid)\n",
    "    return recom_list\n",
    "\n",
    "# the item popularity for each item\n",
    "def item_popularity(ratings, n_users):\n",
    "    item_pop = defaultdict(lambda: 0)\n",
    "    for uid, iid, rtg in ratings: \n",
    "        item_pop[iid] +=1\n",
    "    item_pop.update((iid, float(pop/n_users)) for iid, pop in item_pop.items())\n",
    "    return item_pop\n",
    "\n",
    "def novelty(predictions, k, item_pop, threshold):\n",
    "    '''Return novelty metrics'''\n",
    "    \n",
    "    recom_list = recommendation_list(predictions, k, threshold)\n",
    "  \n",
    "    # novelties: the novelty metrics for each user\n",
    "    novelties = dict()\n",
    "    for uid, items in recom_list.items():\n",
    "        # self_info: define novelty as the negative of the log of the item popularity\n",
    "        self_info = 0\n",
    "        for iid in items:\n",
    "            if item_pop[iid] != 0:\n",
    "                self_info += -math.log2(item_pop[iid])\n",
    "        novelties[uid] = float(self_info/len(items)) \n",
    "\n",
    "    # compute novelty\n",
    "    novelty = sum(nov for nov in novelties.values()) / len(novelties)\n",
    "\n",
    "    return novelty\n",
    "\n",
    "def primitive_list(ratings, k):\n",
    "    # k must be an even number\n",
    "    item_ratings = defaultdict(lambda: 0)\n",
    "    item_pops = defaultdict(lambda: 0)\n",
    "    for uid, iid, rtg in ratings:\n",
    "        item_ratings[iid] += rtg \n",
    "        item_pops[iid] +=1\n",
    "    item_ratings.update((iid, float(rtg/item_pops[iid])) for iid, rtg in item_ratings.items())\n",
    "    \n",
    "    primitive_list = []\n",
    "    a = Counter(item_ratings)\n",
    "    for iid, v in a.most_common(int(k)):\n",
    "        primitive_list.append(iid)\n",
    "    #b = Counter(item_pops)\n",
    "    #for iid, v in b.most_common(int(k/2)):\n",
    "    #    primitive_list.append(iid)\n",
    "    return primitive_list\n",
    "\n",
    "def serendipity(predictions, primitive_list, k, threshold):\n",
    "    '''Return serendipity metrics'''\n",
    "    recom_list = recommendation_list(predictions, k, threshold)\n",
    "    \n",
    "    # novelties: the serendipity metrics for each user\n",
    "    serendipities = dict()\n",
    "    for uid, items in recom_list.items():\n",
    "        # unexpected: the number of unexpected items for each user\n",
    "        n_unexpected = 0\n",
    "        for iid in items:\n",
    "            if iid not in primitive_list:\n",
    "                n_unexpected += 1\n",
    "        serendipities[uid] = float(n_unexpected/len(items))\n",
    "\n",
    "    # compute serendipity\n",
    "    serendipity = sum(ser for ser in serendipities.values()) / len(serendipities)\n",
    "\n",
    "    return serendipity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user based knn\n",
    "def hybrid_switching_knn(predictions_knn, predictions_cb, threshold_knn, item_based = True):\n",
    "    if item_based == True:\n",
    "        df_knn_pred = pd.DataFrame(predictions_knn)\n",
    "        # some ratings don't have actual estimated ratings because those items don't have enough neighbors('was_impossible':True, and algs gives average ratings instead)\n",
    "        # so we only use knn to compute estimated ratings for those item that have enough neighbors \n",
    "        criterion_1 = df_knn_pred['details'].map(lambda x: x['was_impossible'] == False)\n",
    "        criterion_2 = df_knn_pred['details'][criterion_1].map(lambda x: x['actual_k'] >= threshold_knn)\n",
    "        criterion = criterion_1 & criterion_2\n",
    "        sub_knn_pred = df_knn_pred[criterion]\n",
    "        predictions = [tuple(x) for x in sub_knn_pred.values]\n",
    "\n",
    "        iid_knn = sub_knn_pred['iid'].unique()\n",
    "        predictions.extend([(uid, iid, r_ui, est, details) for uid, iid, r_ui, est, details in predictions_cb if iid not in iid_knn])\n",
    "        return predictions\n",
    "    elif item_based == False:    \n",
    "        df_knn_pred = pd.DataFrame(predictions_knn)\n",
    "        criterion = df_knn_pred['details'].map(lambda x: x['actual_k'] >= threshold_knn)\n",
    "        sub_knn_pred = df_knn_pred[criterion]\n",
    "        predictions = [tuple(x) for x in sub_knn_pred.values]\n",
    "\n",
    "        uid_knn = sub_knn_pred['uid'].unique()\n",
    "        predictions.extend([(uid, iid, r_ui, est, details) for uid, iid, r_ui, est, details in predictions_cb if uid not in uid_knn])\n",
    "        return predictions\n",
    "    \n",
    "    else:\n",
    "        print('Error: input for item_based')\n",
    "        return 0 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hybrid_switching_svd(predictions_svd, predictions_cb, threshold_svd):\n",
    "    # use content-based model when the number of available ratings of a user falls below a fixed threshold\n",
    "    # Ids that we apply Matrix Factorization method to recommend books\n",
    "    uid_svd = pd.DataFrame(predictions_svd).groupby('uid').filter(lambda x: len(x) >= threshold_svd)['uid'].unique()\n",
    "    predictions = [(uid, iid, r_ui, est, details) for uid, iid, r_ui, est, details in predictions_svd if uid in uid_svd]\n",
    "    predictions.extend([(uid, iid, r_ui, est, details) for uid, iid, r_ui, est, details in predictions_cb if uid not in uid_svd])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hybrid_mixed(predictions_cf, predictions_cb, n_cf, n_cb):\n",
    "    # choose top n_cf estimated ratings from predictions_cf\n",
    "    sub_cf_pred = pd.DataFrame(predictions_cf).groupby('uid').apply(lambda x: x.nlargest(n_cf, 'est')).reset_index(drop=True)\n",
    "    # choose top n_cb estimated ratings from predictions_cb\n",
    "    sub_cb_pred = pd.DataFrame(predictions_cb).groupby('uid').apply(lambda x: x.nlargest(n_cb, 'est')).reset_index(drop=True)\n",
    "    predictions_df = sub_cf_pred.iloc[:,0:4].merge(sub_cb_pred.iloc[:,0:4], how='outer')\n",
    "    predictions_df['details'] = 0\n",
    "    predictions = [tuple(x) for x in predictions_df.values]\n",
    "    return predictions\n",
    "    # Note there might be problem with metrics since we only have n_cf+n_cb predictions for each user now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hybrid_weighted(predictions_cf, predictions_cb, alpha):\n",
    "    predictions_df = pd.merge(pd.DataFrame(predictions_cf, columns = ['uid', 'iid', 'r_ui', 'est_cf', 'details_cf']), \n",
    "                              pd.DataFrame(predictions_cb, columns = ['uid', 'iid', 'r_ui', 'est_cb', 'details_cb']), \n",
    "                              how='inner', on=['uid', 'iid', 'r_ui'])\n",
    "    predictions_df['est'] = alpha*predictions_df['est_cf'] + (1-alpha)*predictions_df['est_cb']\n",
    "    predictions_df = predictions_df.drop(['est_cf', 'est_cb', 'details_cf', 'details_cb'], axis=1)\n",
    "    predictions_df['details'] = 0\n",
    "    predictions = [tuple(x) for x in predictions_df.values]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "n_items_train = len(np.unique(train['ISBN']))\n",
    "n_users_train = len(np.unique(train['User-ID']))\n",
    "\n",
    "reader = surprise.Reader(rating_scale=(1, 10))\n",
    "data = surprise.Dataset.load_from_df(train[['User-ID', 'ISBN', 'Book-Rating']], reader)\n",
    "trainset= data.build_full_trainset()\n",
    "item_pop_train = item_popularity(trainset.build_testset(), n_users_train)\n",
    "primitive_list_train = primitive_list(trainset.build_testset(), 200)\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "n_items_test = len(np.unique(test['ISBN']))\n",
    "n_users_test = len(np.unique(test['User-ID']))\n",
    "t = [tuple(x) for x in test[['User-ID', 'ISBN', 'Book-Rating']].values]\n",
    "item_pop_test = item_popularity(t, n_users_test)\n",
    "primitive_list_test = primitive_list(t, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "\n",
      " Baseline Training Set:\n",
      " Precision:0.7564715005685758\n",
      " Recall:0.9801762822359228\n",
      " F-Score:0.8539157224469934\n",
      " NDCG:0.9501551494909518\n",
      " Item-space coverage:0.8200587084148728\n",
      " User-space coverage:0.06189063162356931\n",
      " Novelty:11.84878890502456\n",
      " Serendipity:0.9959239620900209\n",
      "\n",
      "\n",
      " Baseline Test Set:\n",
      " Precision:0.7567175847474973\n",
      " Recall:0.98956254621546\n",
      " F-Score:0.8576165606555032\n",
      " NDCG:0.9344864959697291\n",
      " Item-space coverage:0.9199113643558088\n",
      " User-space coverage:0.007220216606498195\n",
      " Novelty:10.677697484759817\n",
      " Serendipity:0.9943203071983051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "algo_baseline = surprise.BaselineOnly()\n",
    "# retrain on the whole train set\n",
    "algo_baseline.train(trainset)\n",
    "\n",
    "# Compute biased accuracy on train set\n",
    "predictions_base_train= algo_baseline.test(trainset.build_testset())\n",
    "precision_base_train, recall_base_train, f_base_train = precision_recall_at_k(predictions_base_train, k=10, threshold=7)\n",
    "print (\"\\n Baseline Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_base_train, recall_base_train, f_base_train, \n",
    "    ndcg_at_k(predictions_base_train, 10),\n",
    "    item_space_coverage(predictions_base_train, 10, n_items_train,7),\n",
    "    user_space_coverage(predictions_base_train, 10, n_users_train,7),\n",
    "    novelty(predictions_base_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_base_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "# Compute unbiased accuracy on test set\n",
    "predictions_base_test = algo_baseline.test(t)\n",
    "precision_base_test, recall_base_test, f_base_test = precision_recall_at_k(predictions_base_test, k=10, threshold=7)\n",
    "print (\"\\n Baseline Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_base_test, recall_base_test, f_base_test, \n",
    "    ndcg_at_k(predictions_base_test, 10),\n",
    "    item_space_coverage(predictions_base_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_base_test, 10, n_users_test,7),\n",
    "    novelty(predictions_base_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_base_test, primitive_list_test, 10, 7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prdictions from content based model has been saved into predictions_cb_train/test.csv\n",
    "predictions_cb_train= pd.read_csv('predictions_cb_train.csv')\n",
    "predictions_cb_train.columns = ['uid', 'iid', 'r_ui', 'est', 'details']\n",
    "predictions_cb_train = [x for x in predictions_cb_train.itertuples(index = False)]\n",
    "\n",
    "predictions_cb_test= pd.read_csv('predictions_cb_test.csv')\n",
    "predictions_cb_test.columns = ['uid', 'iid', 'r_ui', 'est', 'details']\n",
    "predictions_cb_test = [x for x in predictions_cb_test.itertuples(index = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Content Based on Training Set:\n",
      " Precision:0.8681187212768389\n",
      " Recall:0.4802840602502648\n",
      " F-Score:0.6184258738504054\n",
      " NDCG:0.8398460490949666\n",
      " Item-space coverage:0.3311154598825832\n",
      " User-space coverage:0.0038151759220008477\n",
      " Novelty:12.117384069477904\n",
      " Serendipity:0.9921778711484593\n",
      "\n",
      "\n",
      " Content Based on Test Set:\n",
      " Precision:0.8411380436651194\n",
      " Recall:0.5642715270422853\n",
      " F-Score:0.6754333515935501\n",
      " NDCG:0.9074555518057316\n",
      " Item-space coverage:0.4491927825261159\n",
      " User-space coverage:0.0015471892728210418\n",
      " Novelty:10.840945843761158\n",
      " Serendipity:0.9940549225825299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision_cb_train, recall_cb_train, f_cb_train = precision_recall_at_k(predictions_cb_train, k=10, threshold=7)\n",
    "print (\"\\n Content Based on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_cb_train, recall_cb_train, f_cb_train, \n",
    "    ndcg_at_k(predictions_cb_train, 10),\n",
    "    item_space_coverage(predictions_cb_train, 10, n_items_train,7),\n",
    "    user_space_coverage(predictions_cb_train, 10, n_users_train,7),\n",
    "    novelty(predictions_cb_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_cb_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "precision_cb_test, recall_cb_test, f_cb_test = precision_recall_at_k(predictions_cb_test, k=10, threshold=7)\n",
    "print (\"\\n Content Based on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_cb_test, recall_cb_test, f_cb_test, \n",
    "    ndcg_at_k(predictions_cb_test, 10),\n",
    "    item_space_coverage(predictions_cb_test, 10, n_items_test,7),\n",
    "    user_space_coverage(predictions_cb_test, 10, n_users_test,7),\n",
    "    novelty(predictions_cb_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_cb_test, primitive_list_test, 10, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      " KNN on Training Set:\n",
      " Precision:0.999725637040177\n",
      " Recall:0.9848647121843231\n",
      " F-Score:0.9922395341402358\n",
      " NDCG:0.9990520972268696\n",
      " Item-space coverage:0.6845401174168297\n",
      " User-space coverage:0.04980924120389996\n",
      " Novelty:11.770722106683836\n",
      " Serendipity:0.9944185829602498\n",
      "\n",
      "\n",
      " KNN on Test Set:\n",
      " Precision:0.7469111471303325\n",
      " Recall:0.9977161491339762\n",
      " F-Score:0.8542859727757216\n",
      " NDCG:0.9181076071711439\n",
      " Item-space coverage:0.9487179487179487\n",
      " User-space coverage:0.008767405879319236\n",
      " Novelty:10.675132378262889\n",
      " Serendipity:0.9941755282136922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#knn\n",
    "sim_options = {'name': 'pearson',\n",
    "               'user_based': False\n",
    "               }\n",
    "algo_knn = surprise.KNNBasic(k=5, sim_options=sim_options)\n",
    "# retrain on the whole train set\n",
    "algo_knn.train(trainset)\n",
    "\n",
    "# Compute biased accuracy on train set\n",
    "predictions_knn_train = algo_knn.test(trainset.build_testset())\n",
    "precision_knn_train, recall_knn_train, f_knn_train = precision_recall_at_k(predictions_knn_train, k=10, threshold=7)\n",
    "print (\"\\n KNN on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_train, recall_knn_train, f_knn_train, \n",
    "    ndcg_at_k(predictions_knn_train, 10),\n",
    "    item_space_coverage(predictions_knn_train, 10, n_items_train,7),\n",
    "    user_space_coverage(predictions_knn_train, 10, n_users_train,7),\n",
    "    novelty(predictions_knn_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_knn_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "# Compute unbiased accuracy on test set\n",
    "predictions_knn_test = algo_knn.test(t)\n",
    "precision_knn_test, recall_knn_test, f_knn_test = precision_recall_at_k(predictions_knn_test, k=10, threshold=7)\n",
    "print (\"\\n KNN on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_test, recall_knn_test, f_knn_test, \n",
    "    ndcg_at_k(predictions_knn_test, 10),\n",
    "    item_space_coverage(predictions_knn_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_knn_test, 10, n_users_test,7),\n",
    "    novelty(predictions_knn_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_knn_test, primitive_list_test, 10, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HS KNN on Training Set:\n",
      " Precision:0.8690834032177125\n",
      " Recall:0.4947138815160033\n",
      " F-Score:0.63051544181791\n",
      " NDCG:0.8533649385716962\n",
      " Item-space coverage:0.3519569471624266\n",
      " User-space coverage:0.00635862653666808\n",
      " Novelty:12.014299395848248\n",
      " Serendipity:0.9940647799482232\n",
      "\n",
      "\n",
      " HS KNN on Test Set:\n",
      " Precision:0.8411380436651194\n",
      " Recall:0.5642822714122353\n",
      " F-Score:0.6754410488572925\n",
      " NDCG:0.907610663855474\n",
      " Item-space coverage:0.4491927825261159\n",
      " User-space coverage:0.0015471892728210418\n",
      " Novelty:10.840945843761158\n",
      " Serendipity:0.9940549225825299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold_knn = 2\n",
    "\n",
    "predictions_knn_hs_train = hybrid_switching_knn(predictions_knn_train, predictions_cb_train, threshold_knn, item_based = True)\n",
    "precision_knn_hs_train, recall_knn_hs_train, f_knn_hs_train = precision_recall_at_k(predictions_knn_hs_train, k=10, threshold=7)\n",
    "print (\"\\n HS KNN on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hs_train, recall_knn_hs_train, f_knn_hs_train, \n",
    "    ndcg_at_k(predictions_knn_hs_train, k=10),\n",
    "    item_space_coverage(predictions_knn_hs_train, 10, n_items_train,7), \n",
    "    user_space_coverage(predictions_knn_hs_train, 10, n_users_train,7),\n",
    "    novelty(predictions_knn_hs_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_knn_hs_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_knn_hs_test = hybrid_switching_knn(predictions_knn_test, predictions_cb_test, threshold_knn, item_based = True)\n",
    "precision_knn_hs_test, recall_knn_hs_test, f_knn_hs_test = precision_recall_at_k(predictions_knn_hs_test, k=10, threshold=7)\n",
    "print (\"\\n HS KNN on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hs_test, recall_knn_hs_test, f_knn_hs_test, \n",
    "    ndcg_at_k(predictions_knn_hs_test, 10),\n",
    "    item_space_coverage(predictions_knn_hs_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_knn_hs_test, 10, n_users_test,7),\n",
    "    novelty(predictions_knn_hs_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_knn_hs_test, primitive_list_test, 10, 7)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HW KNN on Training Set:\n",
      " Precision:0.9222149888639924\n",
      " Recall:0.6170098819819179\n",
      " F-Score:0.7393536476943934\n",
      " NDCG:0.8943515670097424\n",
      " Item-space coverage:0.43228962818003913\n",
      " User-space coverage:0.0072064434082238235\n",
      " Novelty:12.094779824399536\n",
      " Serendipity:0.9895595560261993\n",
      "\n",
      "\n",
      " HW KNN on Test Set:\n",
      " Precision:0.818009610583102\n",
      " Recall:0.6278214133487472\n",
      " F-Score:0.710406598486918\n",
      " NDCG:0.907505276522854\n",
      " Item-space coverage:0.5254827477049699\n",
      " User-space coverage:0.0020629190304280558\n",
      " Novelty:10.831169571028939\n",
      " Serendipity:0.9953944020356235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tunning on alpha\n",
    "#alphas_knn = [0.2, 0.4, 0.6, 0.8]\n",
    "#for alpha_knn in alphas_knn:\n",
    "#    print ('\\n alpha:{}'.format(alpha_knn))\n",
    "alpha_knn = 0.5\n",
    "\n",
    "predictions_knn_hw_train = hybrid_weighted(predictions_knn_train, predictions_cb_train, alpha_knn)\n",
    "precision_knn_hw_train, recall_knn_hw_train, f_knn_hw_train = precision_recall_at_k(predictions_knn_hw_train, k=10, threshold=7)\n",
    "print (\"\\n HW KNN on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hw_train, recall_knn_hw_train, f_knn_hw_train, \n",
    "    ndcg_at_k(predictions_knn_hw_train, k=10),\n",
    "    item_space_coverage(predictions_knn_hw_train, 10, n_items_train,7), \n",
    "    user_space_coverage(predictions_knn_hw_train, 10, n_users_train,7),\n",
    "    novelty(predictions_knn_hw_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_knn_hw_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_knn_hw_test = hybrid_weighted(predictions_knn_test, predictions_cb_test, alpha_knn)\n",
    "precision_knn_hw_test, recall_knn_hw_test, f_knn_hw_test = precision_recall_at_k(predictions_knn_hw_test, k=10, threshold=7)\n",
    "print (\"\\n HW KNN on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hw_test, recall_knn_hw_test, f_knn_hw_test, \n",
    "    ndcg_at_k(predictions_knn_hw_test, 10),\n",
    "    item_space_coverage(predictions_knn_hw_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_knn_hw_test, 10, n_users_test,7),\n",
    "    novelty(predictions_knn_hw_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_knn_hw_test, primitive_list_test, 10, 7)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HM KNN on Training Set:\n",
      " Precision:0.8941008599285417\n",
      " Recall:0.7353715391945402\n",
      " F-Score:0.8070051704031961\n",
      " NDCG:0.8939839049790624\n",
      " Item-space coverage:0.7562622309197652\n",
      " User-space coverage:0.06782534972445951\n",
      " Novelty:11.851056722625783\n",
      " Serendipity:0.9948425084319445\n",
      "\n",
      "\n",
      " HM KNN on Test Set:\n",
      " Precision:0.746691348019352\n",
      " Recall:0.7797254728684226\n",
      " F-Score:0.7628509545414067\n",
      " NDCG:0.8991725008366419\n",
      " Item-space coverage:0.9392212725546059\n",
      " User-space coverage:0.024239298607529654\n",
      " Novelty:10.684740657283989\n",
      " Serendipity:0.9945956842424094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_knn = 10\n",
    "n_cb = 10\n",
    "\n",
    "predictions_knn_hm_train = hybrid_mixed(predictions_knn_train, predictions_cb_train, n_knn, n_cb)\n",
    "precision_knn_hm_train, recall_knn_hm_train, f_knn_hm_train = precision_recall_at_k(predictions_knn_hm_train, k=10, threshold=7)\n",
    "print (\"\\n HM KNN on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hm_train, recall_knn_hm_train, f_knn_hm_train, \n",
    "    ndcg_at_k(predictions_knn_hm_train, 10), \n",
    "    item_space_coverage(predictions_knn_hm_train, 10, n_items_train,7), \n",
    "    user_space_coverage(predictions_knn_hm_train, 10, n_users_train,7),\n",
    "    novelty(predictions_knn_hm_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_knn_hm_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_knn_hm_test = hybrid_mixed(predictions_knn_test, predictions_cb_test, n_knn, n_cb)\n",
    "precision_knn_hm_test, recall_knn_hm_test, f_knn_hm_test = precision_recall_at_k(predictions_knn_hm_test, k=10, threshold=7)\n",
    "print (\"\\n HM KNN on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_knn_hm_test, recall_knn_hm_test, f_knn_hm_test, \n",
    "    ndcg_at_k(predictions_knn_hm_test, 10), \n",
    "    item_space_coverage(predictions_knn_hm_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_knn_hm_test, 10, n_users_test,7),\n",
    "    novelty(predictions_knn_hm_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_knn_hm_test, primitive_list_test, 10, 7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SVD on Training Set:\n",
      " Precision:0.7406566879966627\n",
      " Recall:0.983723350513273\n",
      " F-Score:0.8450588181544213\n",
      " NDCG:0.9396202946133264\n",
      " Item-space coverage:0.8423679060665362\n",
      " User-space coverage:0.06718948707079271\n",
      " Novelty:11.850614597497001\n",
      " Serendipity:0.9962721468287626\n",
      "\n",
      "\n",
      " SVD on Test Set:\n",
      " Precision:0.7479426066455465\n",
      " Recall:0.9969018509584953\n",
      " F-Score:0.8546611312271732\n",
      " NDCG:0.9316886810628824\n",
      " Item-space coverage:0.9414371636593859\n",
      " User-space coverage:0.007735946364105209\n",
      " Novelty:10.674805360622406\n",
      " Serendipity:0.994376019208234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVD\n",
    "algo_svd = surprise.SVD(n_factors = 10, lr_all= 0.001, reg_all =1)\n",
    "# retrain on the whole train set\n",
    "algo_svd.train(trainset)\n",
    "\n",
    "# Compute biased accuracy on train set\n",
    "predictions_svd_train = algo_svd.test(trainset.build_testset())\n",
    "precision_svd_train, recall_svd_train, f_svd_train = precision_recall_at_k(predictions_svd_train, k=10, threshold=7)\n",
    "print (\"\\n SVD on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_train, recall_svd_train, f_svd_train, \n",
    "    ndcg_at_k(predictions_svd_train, 10),\n",
    "    item_space_coverage(predictions_svd_train, 10, n_items_train,7),\n",
    "    user_space_coverage(predictions_svd_train, 10, n_users_train,7),\n",
    "    novelty(predictions_svd_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_svd_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "# Compute unbiased accuracy on test set\n",
    "predictions_svd_test = algo_svd.test(t)\n",
    "precision_svd_test, recall_svd_test, f_svd_test = precision_recall_at_k(predictions_svd_test, k=10, threshold=7)\n",
    "print (\"\\n SVD on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_test, recall_svd_test, f_svd_test, \n",
    "    ndcg_at_k(predictions_svd_test, 10),\n",
    "    item_space_coverage(predictions_svd_test, 10, n_items_test,7),\n",
    "    user_space_coverage(predictions_svd_test, 10, n_users_test,7),\n",
    "    novelty(predictions_svd_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_svd_test, primitive_list_test, 10, 7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HS SVD on Training Set:\n",
      " Precision:0.8676349273636257\n",
      " Recall:0.6124502121778201\n",
      " F-Score:0.7180440924112936\n",
      " NDCG:0.9277473849231519\n",
      " Item-space coverage:0.5935420743639922\n",
      " User-space coverage:0.06718948707079271\n",
      " Novelty:11.759606538468821\n",
      " Serendipity:0.9937816877202029\n",
      "\n",
      "\n",
      " HS SVD on Test Set:\n",
      " Precision:0.8410748053734128\n",
      " Recall:0.6018528566315228\n",
      " F-Score:0.7016336128055448\n",
      " NDCG:0.9174858216387138\n",
      " Item-space coverage:0.5837290281734726\n",
      " User-space coverage:0.007735946364105209\n",
      " Novelty:10.813965303749805\n",
      " Serendipity:0.993298365891601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold_svd = 5\n",
    "\n",
    "predictions_svd_hs_train = hybrid_switching_svd(predictions_svd_train, predictions_cb_train, threshold_svd)\n",
    "precision_svd_hs_train, recall_svd_hs_train, f_svd_hs_train= precision_recall_at_k(predictions_svd_hs_train, k=10, threshold=7)\n",
    "print (\"\\n HS SVD on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hs_train, recall_svd_hs_train, f_svd_hs_train, \n",
    "    ndcg_at_k(predictions_svd_hs_train, 10), \n",
    "    item_space_coverage(predictions_svd_hs_train, 10, n_items_train,7), \n",
    "    user_space_coverage(predictions_svd_hs_train, 10, n_users_train,7),\n",
    "    novelty(predictions_svd_hs_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_svd_hs_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_svd_hs_test = hybrid_switching_svd(predictions_svd_test, predictions_cb_test, threshold_svd)\n",
    "precision_svd_hs_test, recall_svd_hs_test, f_svd_hs_test = precision_recall_at_k(predictions_svd_hs_test, k=10, threshold=7)\n",
    "print (\"\\n HS SVD on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hs_test, recall_svd_hs_test, f_svd_hs_test, \n",
    "    ndcg_at_k(predictions_svd_hs_test, 10), \n",
    "    item_space_coverage(predictions_svd_hs_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_svd_hs_test, 10, n_users_test,7),\n",
    "    novelty(predictions_svd_hs_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_svd_hs_test, primitive_list_test, 10, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HW SVD on Training Set:\n",
      " Precision:0.8460363720166602\n",
      " Recall:0.5507498869051338\n",
      " F-Score:0.6671807275158684\n",
      " NDCG:0.842561781798418\n",
      " Item-space coverage:0.40821917808219177\n",
      " User-space coverage:0.006146672318779143\n",
      " Novelty:12.115399648619691\n",
      " Serendipity:0.9921132357570034\n",
      "\n",
      "\n",
      " HW SVD on Test Set:\n",
      " Precision:0.8178708547197457\n",
      " Recall:0.6265320889547296\n",
      " F-Score:0.7095282342739908\n",
      " NDCG:0.90868545844522\n",
      " Item-space coverage:0.5245330800886356\n",
      " User-space coverage:0.0020629190304280558\n",
      " Novelty:10.829375558201301\n",
      " Serendipity:0.9954185809086269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha_svd = 0.5\n",
    "\n",
    "predictions_svd_hw_train = hybrid_weighted(predictions_svd_train, predictions_cb_train, alpha_svd)\n",
    "precision_svd_hw_train, recall_svd_hw_train, f_svd_hw_train= precision_recall_at_k(predictions_svd_hw_train, k=10, threshold=7)\n",
    "print (\"\\n HW SVD on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hw_train, recall_svd_hw_train, f_svd_hw_train, \n",
    "    ndcg_at_k(predictions_svd_hw_train, 10), \n",
    "    item_space_coverage(predictions_svd_hw_train, 10, n_items_train,7), \n",
    "    user_space_coverage(predictions_svd_hw_train, 10, n_users_train,7),\n",
    "    novelty(predictions_svd_hw_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_svd_hw_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_svd_hw_test = hybrid_weighted(predictions_svd_test, predictions_cb_test, alpha_svd)\n",
    "precision_svd_hw_test, recall_svd_hw_test, f_svd_hw_test = precision_recall_at_k(predictions_svd_hw_test, k=10, threshold=7)\n",
    "print (\"\\n HW SVD on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hw_test, recall_svd_hw_test, f_svd_hw_test, \n",
    "    ndcg_at_k(predictions_svd_hw_test, 10), \n",
    "    item_space_coverage(predictions_svd_hw_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_svd_hw_test, 10, n_users_test,7),\n",
    "    novelty(predictions_svd_hw_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_svd_hw_test, primitive_list_test, 10, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HM SVD on Training Set:\n",
      " Precision:0.7392111938741871\n",
      " Recall:0.7350142711061448\n",
      " F-Score:0.7371067584512081\n",
      " NDCG:0.8523596050344521\n",
      " Item-space coverage:0.8495107632093933\n",
      " User-space coverage:0.08986858838490885\n",
      " Novelty:11.881162758033005\n",
      " Serendipity:0.9959211429378879\n",
      "\n",
      "\n",
      " HM SVD on Test Set:\n",
      " Precision:0.7467601119870328\n",
      " Recall:0.779774684907214\n",
      " F-Score:0.7629103931475\n",
      " NDCG:0.9047600886328183\n",
      " Item-space coverage:0.9398543842988287\n",
      " User-space coverage:0.02372356884992264\n",
      " Novelty:10.685523351542422\n",
      " Serendipity:0.9945956842424094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_svd = 10\n",
    "n_cb = 10\n",
    "\n",
    "predictions_svd_hm_train = hybrid_mixed(predictions_svd_train, predictions_cb_train, n_svd, n_cb)\n",
    "precision_svd_hm_train, recall_svd_hm_train, f_svd_hm_train = precision_recall_at_k(predictions_svd_hm_train, k=10, threshold=7)\n",
    "print (\"\\n HM SVD on Training Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hm_train, recall_svd_hm_train, f_svd_hm_train, \n",
    "    ndcg_at_k(predictions_svd_hm_train, 10),\n",
    "    item_space_coverage(predictions_svd_hm_train, 10, n_items_train,7),\n",
    "    user_space_coverage(predictions_svd_hm_train, 10, n_users_train,7),\n",
    "    novelty(predictions_svd_hm_train, 10, item_pop_train, 7),\n",
    "    serendipity(predictions_svd_hm_train, primitive_list_train, 10, 7)))\n",
    "\n",
    "predictions_svd_hm_test = hybrid_mixed(predictions_svd_test, predictions_cb_test, n_svd, n_cb)\n",
    "precision_svd_hm_test, recall_svd_hm_test, f_svd_hm_test = precision_recall_at_k(predictions_svd_hm_test, k=10, threshold=7)\n",
    "print (\"\\n HM SVD on Test Set:\\n Precision:{}\\n Recall:{}\\n F-Score:{}\\n NDCG:{}\\n Item-space coverage:{}\\n User-space coverage:{}\\n Novelty:{}\\n Serendipity:{}\\n\".format(precision_svd_hm_test, recall_svd_hm_test, f_svd_hm_test, \n",
    "    ndcg_at_k(predictions_svd_hm_test, 10),\n",
    "    item_space_coverage(predictions_svd_hm_test, 10, n_items_test,7), \n",
    "    user_space_coverage(predictions_svd_hm_test, 10, n_users_test,7),\n",
    "    novelty(predictions_svd_hm_test, 10, item_pop_test, 7),\n",
    "    serendipity(predictions_svd_hm_test, primitive_list_test, 10, 7)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
